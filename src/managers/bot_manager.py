import asyncio
from typing import Optional, List, Dict
import ollama
from datetime import datetime


class BotManager:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒœãƒƒãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, default_model: str = "gemma3:4b", bot_client_id: str = "bot"):
        """
        åˆæœŸåŒ–
        
        Args:
            default_model: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemma3:4bï¼‰
            bot_client_id: ãƒœãƒƒãƒˆã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆID
        """
        self.default_model = default_model
        self.bot_client_id = bot_client_id
        self.conversation_history: Dict[str, List[Dict]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®ä¼šè©±å±¥æ­´
        self.system_prompts: Dict[str, str] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.models: Dict[str, str] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®ãƒ¢ãƒ‡ãƒ«
        self.temperatures: Dict[str, float] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®temperature
        self.top_ps: Dict[str, float] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®top_p
        self.top_ks: Dict[str, int] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®top_k
        self.repeat_penalties: Dict[str, float] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®repeat_penalty
        self.num_predicts: Dict[str, Optional[int]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®num_predict
        self.num_threads: Dict[str, Optional[int]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®num_thread
        self.num_ctxs: Dict[str, Optional[int]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®num_ctx
        self.num_gpus: Dict[str, Optional[int]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®num_gpu
        self.num_batches: Dict[str, Optional[int]] = {}  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã”ã¨ã®num_batch
        self.default_system_prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        self.default_temperature = 0.7
        self.default_top_p = 0.9
        self.default_top_k = 40
        self.default_repeat_penalty = 1.1
        self.default_num_predict = None
        # M4æœ€é©åŒ–ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        self.default_num_thread = 8  # M4ã®10ã‚³ã‚¢ã‚’æ´»ç”¨
        self.default_num_ctx = 8192  # 16GBãƒ¡ãƒ¢ãƒªã§ä½™è£•ã‚’æŒãŸã›ã‚‹
        self.default_num_gpu = -1  # å…¨GPUãƒ¬ã‚¤ãƒ¤ãƒ¼ä½¿ç”¨ï¼ˆM4 Neural Engineï¼‰
        self.default_num_batch = 512  # ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
    
    def set_model(self, session_id: str, model: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
        self.models[session_id] = model
    
    def get_model(self, session_id: str) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        return self.models.get(session_id, self.default_model)
    
    def set_system_prompt(self, session_id: str, prompt: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š"""
        self.system_prompts[session_id] = prompt
    
    def get_system_prompt(self, session_id: str) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        return self.system_prompts.get(session_id, self.default_system_prompt)
    
    def set_temperature(self, session_id: str, temperature: float):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®temperatureã‚’è¨­å®š"""
        self.temperatures[session_id] = temperature
    
    def get_temperature(self, session_id: str) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®temperatureã‚’å–å¾—"""
        return self.temperatures.get(session_id, self.default_temperature)
    
    def set_top_p(self, session_id: str, top_p: float):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®top_pã‚’è¨­å®š"""
        self.top_ps[session_id] = top_p
    
    def get_top_p(self, session_id: str) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®top_pã‚’å–å¾—"""
        return self.top_ps.get(session_id, self.default_top_p)
    
    def set_top_k(self, session_id: str, top_k: int):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®top_kã‚’è¨­å®š"""
        self.top_ks[session_id] = top_k
    
    def get_top_k(self, session_id: str) -> int:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®top_kã‚’å–å¾—"""
        return self.top_ks.get(session_id, self.default_top_k)
    
    def set_repeat_penalty(self, session_id: str, repeat_penalty: float):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®repeat_penaltyã‚’è¨­å®š"""
        self.repeat_penalties[session_id] = repeat_penalty
    
    def get_repeat_penalty(self, session_id: str) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®repeat_penaltyã‚’å–å¾—"""
        return self.repeat_penalties.get(session_id, self.default_repeat_penalty)
    
    def set_num_predict(self, session_id: str, num_predict: Optional[int]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_predictã‚’è¨­å®š"""
        self.num_predicts[session_id] = num_predict
    
    def get_num_predict(self, session_id: str) -> Optional[int]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_predictã‚’å–å¾—"""
        return self.num_predicts.get(session_id, self.default_num_predict)
    
    def set_num_thread(self, session_id: str, num_thread: Optional[int]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_threadã‚’è¨­å®š"""
        self.num_threads[session_id] = num_thread
    
    def get_num_thread(self, session_id: str) -> Optional[int]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_threadã‚’å–å¾—"""
        return self.num_threads.get(session_id, self.default_num_thread)
    
    def set_num_ctx(self, session_id: str, num_ctx: Optional[int]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_ctxã‚’è¨­å®š"""
        self.num_ctxs[session_id] = num_ctx
    
    def get_num_ctx(self, session_id: str) -> Optional[int]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_ctxã‚’å–å¾—"""
        return self.num_ctxs.get(session_id, self.default_num_ctx)
    
    def set_num_gpu(self, session_id: str, num_gpu: Optional[int]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_gpuã‚’è¨­å®š"""
        self.num_gpus[session_id] = num_gpu
    
    def get_num_gpu(self, session_id: str) -> Optional[int]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_gpuã‚’å–å¾—"""
        return self.num_gpus.get(session_id, self.default_num_gpu)
    
    def set_num_batch(self, session_id: str, num_batch: Optional[int]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_batchã‚’è¨­å®š"""
        self.num_batches[session_id] = num_batch
    
    def get_num_batch(self, session_id: str) -> Optional[int]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®num_batchã‚’å–å¾—"""
        return self.num_batches.get(session_id, self.default_num_batch)
    
    def get_conversation_history(self, session_id: str) -> List[Dict]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []
        return self.conversation_history[session_id]
    
    def add_to_history(self, session_id: str, role: str, content: str):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        
        æ³¨æ„: ã“ã®å±¥æ­´ã¯AIãŒå‚ç…§ã™ã‚‹ä¼šè©±å±¥æ­´ã§ã™ã€‚
        æ•™ç¤ºæ–‡(instruction)ã‚„ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸(system)ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
        """
        history = self.get_conversation_history(session_id)
        history.append({
            "role": role,
            "content": content
        })
        
        # å±¥æ­´ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«åˆ¶é™ï¼ˆæœ€æ–°100ä»¶ã¾ã§ä¿æŒï¼‰
        if len(history) > 100:
            self.conversation_history[session_id] = history[-100:]
    
    def clear_history(self, session_id: str):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]
    
    async def generate_response(self, user_message: str, session_id: str, 
                               client_id: str, timeout: float = 300.0) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            client_id: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ç§’ = 5åˆ†ï¼‰
            
        Returns:
            ãƒœãƒƒãƒˆã®å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.add_to_history(session_id, "user", user_message)
            
            # ä¼šè©±å±¥æ­´ã‚’å–å¾—
            history = self.get_conversation_history(session_id)
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + ä¼šè©±å±¥æ­´ï¼‰
            messages = [
                {
                    "role": "system",
                    "content": self.get_system_prompt(session_id)
                }
            ]
            messages.extend(history)
            
            # Ollamaã‚’ä½¿ã£ã¦å¿œç­”ã‚’ç”Ÿæˆ
            options = {
                'temperature': self.get_temperature(session_id),
                'top_p': self.get_top_p(session_id),
                'top_k': self.get_top_k(session_id),
                'repeat_penalty': self.get_repeat_penalty(session_id)
            }
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆNoneã§ãªã„å ´åˆã®ã¿ï¼‰
            num_predict = self.get_num_predict(session_id)
            if num_predict is not None:
                options['num_predict'] = num_predict
            
            num_thread = self.get_num_thread(session_id)
            if num_thread is not None:
                options['num_thread'] = num_thread
            
            num_ctx = self.get_num_ctx(session_id)
            if num_ctx is not None:
                options['num_ctx'] = num_ctx
            
            num_gpu = self.get_num_gpu(session_id)
            if num_gpu is not None:
                options['num_gpu'] = num_gpu
            
            num_batch = self.get_num_batch(session_id)
            if num_batch is not None:
                options['num_batch'] = num_batch
            
            # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±ã‚’å‡ºåŠ›
            model = self.get_model(session_id)
            system_prompt = self.get_system_prompt(session_id)
            system_prompt_preview = system_prompt[:80] + "..." if len(system_prompt) > 80 else system_prompt
            
            print("\n" + "=" * 70)
            print("ğŸ¤– OLLAMA MODEL INVOCATION")
            print("=" * 70)
            print(f"Session ID    : {session_id[:20]}...")
            print(f"Model         : {model}")
            print(f"System Prompt : {system_prompt_preview}")
            print(f"\nParameters:")
            print(f"  temperature      : {options.get('temperature', 'N/A')}")
            print(f"  top_p            : {options.get('top_p', 'N/A')}")
            print(f"  top_k            : {options.get('top_k', 'N/A')}")
            print(f"  repeat_penalty   : {options.get('repeat_penalty', 'N/A')}")
            print(f"  num_predict      : {options.get('num_predict', 'Default (unlimited)')}")
            print(f"  num_thread       : {options.get('num_thread', 'Default (8)')}")
            print(f"  num_ctx          : {options.get('num_ctx', 'Default (8192)')}")
            print(f"  num_gpu          : {options.get('num_gpu', 'Default (-1, all)')}")
            print(f"  num_batch        : {options.get('num_batch', 'Default (512)')}")
            print(f"\nConversation History: {len(messages) - 1} messages")
            print(f"Timeout: {timeout}s")
            print("=" * 70 + "\n")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å¿œç­”ã‚’ç”Ÿæˆ
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        ollama.chat,
                        model=model,
                        messages=messages,
                        options=options
                    ),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                print(f"âš ï¸ Response generation timed out after {timeout}s")
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯å±¥æ­´ã‹ã‚‰æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼ˆå¿œç­”ãŒãªã„ãŸã‚ï¼‰
                if history and history[-1].get("role") == "user":
                    history.pop()
                return None  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯Noneã‚’è¿”ã™
            
            bot_message = response['message']['content']
            
            # å¿œç­”ã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
            print(f"âœ… Response generated: {len(bot_message)} chars\n")
            
            # ãƒœãƒƒãƒˆã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
            self.add_to_history(session_id, "assistant", bot_message)
            
            return bot_message
            
        except asyncio.CancelledError:
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ï¼ˆæ¥ç¶šåˆ‡æ–­ãªã©ï¼‰
            print(f"âš ï¸ [BotManager] Response generation cancelled for session {session_id[:12]}...")
            # å±¥æ­´ã‹ã‚‰æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
            history = self.get_conversation_history(session_id)
            if history and history[-1].get("role") == "user":
                history.pop()
            return None  # ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã¯Noneã‚’è¿”ã™
        except Exception as e:
            error_message = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            print(f"[BotManager] Error generating response: {e}")
            return error_message
    
    async def stream_response(self, user_message: str, session_id: str, 
                             client_id: str):
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’ç”Ÿæˆï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            client_id: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
            
        Yields:
            ãƒœãƒƒãƒˆã®å¿œç­”ã®ãƒãƒ£ãƒ³ã‚¯
        """
        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.add_to_history(session_id, "user", user_message)
            
            # ä¼šè©±å±¥æ­´ã‚’å–å¾—
            history = self.get_conversation_history(session_id)
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
            messages = [
                {
                    "role": "system",
                    "content": self.get_system_prompt(session_id)
                }
            ]
            messages.extend(history)
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
            options = {
                'temperature': self.get_temperature(session_id),
                'top_p': self.get_top_p(session_id),
                'top_k': self.get_top_k(session_id),
                'repeat_penalty': self.get_repeat_penalty(session_id)
            }
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆNoneã§ãªã„å ´åˆã®ã¿ï¼‰
            num_predict = self.get_num_predict(session_id)
            if num_predict is not None:
                options['num_predict'] = num_predict
            
            num_thread = self.get_num_thread(session_id)
            if num_thread is not None:
                options['num_thread'] = num_thread
            
            num_ctx = self.get_num_ctx(session_id)
            if num_ctx is not None:
                options['num_ctx'] = num_ctx
            
            num_gpu = self.get_num_gpu(session_id)
            if num_gpu is not None:
                options['num_gpu'] = num_gpu
            
            num_batch = self.get_num_batch(session_id)
            if num_batch is not None:
                options['num_batch'] = num_batch
            
            # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±ã‚’å‡ºåŠ›
            model = self.get_model(session_id)
            system_prompt = self.get_system_prompt(session_id)
            system_prompt_preview = system_prompt[:80] + "..." if len(system_prompt) > 80 else system_prompt
            
            print("\n" + "=" * 70)
            print("ğŸ¤– OLLAMA MODEL INVOCATION (STREAMING)")
            print("=" * 70)
            print(f"Session ID    : {session_id[:20]}...")
            print(f"Model         : {model}")
            print(f"System Prompt : {system_prompt_preview}")
            print(f"\nParameters:")
            print(f"  temperature      : {options.get('temperature', 'N/A')}")
            print(f"  top_p            : {options.get('top_p', 'N/A')}")
            print(f"  top_k            : {options.get('top_k', 'N/A')}")
            print(f"  repeat_penalty   : {options.get('repeat_penalty', 'N/A')}")
            print(f"  num_predict      : {options.get('num_predict', 'Default (unlimited)')}")
            print(f"  num_thread       : {options.get('num_thread', 'Default (8)')}")
            print(f"  num_ctx          : {options.get('num_ctx', 'Default (8192)')}")
            print(f"  num_gpu          : {options.get('num_gpu', 'Default (-1, all)')}")
            print(f"  num_batch        : {options.get('num_batch', 'Default (512)')}")
            print(f"\nConversation History: {len(messages) - 1} messages")
            print("=" * 70 + "\n")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’ç”Ÿæˆ
            full_response = ""
            stream = await asyncio.to_thread(
                ollama.chat,
                model=model,
                messages=messages,
                options=options,
                stream=True
            )
            
            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    content = chunk['message']['content']
                    full_response += content
                    yield content
            
            # å¿œç­”ã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
            print(f"âœ… Streaming response completed: {len(full_response)} chars\n")
            
            # å®Œå…¨ãªå¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
            self.add_to_history(session_id, "assistant", full_response)
            
        except Exception as e:
            error_message = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            print(f"[BotManager] Error in streaming response: {e}")
            yield error_message
    
    def is_bot_message(self, client_id: str) -> bool:
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDãŒãƒœãƒƒãƒˆã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return client_id == self.bot_client_id
    
    @staticmethod
    def get_available_models() -> list:
        """Ollamaã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        try:
            models = ollama.list()
            
            # ollama-python 0.4.x ã¯ ListResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
            if hasattr(models, 'models'):
                # å„ãƒ¢ãƒ‡ãƒ«ã¯ Model ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€.model å±æ€§ã«ãƒ¢ãƒ‡ãƒ«åãŒã‚ã‚‹
                model_list = [model.model for model in models.models]
                print(f"[BotManager] Successfully loaded {len(model_list)} models: {model_list}")
                return model_list
            elif isinstance(models, dict) and 'models' in models:
                # æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œ: è¾æ›¸å½¢å¼
                model_list = [model.get('name', model.get('model', '')) for model in models['models']]
                print(f"[BotManager] Successfully loaded {len(model_list)} models (dict format): {model_list}")
                return model_list
            else:
                print(f"[BotManager] WARNING: Unexpected response format from ollama.list()")
                print(f"[BotManager] Response type: {type(models)}")
                return []
        except Exception as e:
            print(f"[BotManager] Failed to get available models: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def check_model_availability(self, model: str) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸOllamaãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            model: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆTrue
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
            models = await asyncio.to_thread(ollama.list)
            available_models = [model_info['name'] for model_info in models.get('models', [])]
            
            # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            is_available = any(model in model_name for model_name in available_models)
            
            if not is_available:
                print(f"[BotManager] Warning: Model '{model}' not found in available models.")
                print(f"[BotManager] Available models: {available_models}")
                print(f"[BotManager] Attempting to pull model...")
                
                # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«
                await asyncio.to_thread(ollama.pull, model)
                print(f"[BotManager] Successfully pulled model '{model}'")
                return True
            
            return True
            
        except Exception as e:
            print(f"[BotManager] Error checking model availability: {e}")
            return False

